{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#FF0000 size=4 face=\"黑体\">Package import</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import Ipynb_importer # a package that enables us to import function from other notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#FF0000 size=4 face=\"黑体\">Deep neural network package include</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Deep_Neural_Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#FF0000 size=4 face=\"黑体\">Gradient descent</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    L = len(parameters)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads['dW' + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads['db' + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#FF0000 size=4 face=\"黑体\">Mini-batch gradient descent</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    shuffle and partition\n",
    "    Returns:\n",
    "    mini_batches -- lis of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    np.random.random()\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    # shuffle\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "    \n",
    "    # Partition\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    if m % mini_batch_size != 0: # handling with the end case\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#FF0000 size=4 face=\"黑体\">Momentum</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize velocity\n",
    "def initialize_velocity(parameters):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    v -- python dictionary containing the current velocity\n",
    "    \"\"\"\n",
    "    L = len(parameters)\n",
    "    v = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros(parameters[\"W\" + str(l + 1)].shape)\n",
    "        v[\"db\" + str(l + 1)] = np.zeros(parameters[\"b\" + str(l + 1)].shape)\n",
    "    \n",
    "    return v\n",
    "\n",
    "# update parameters with momentum\n",
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing updated parameters\n",
    "    v -- python dictionary containing updated velocity\n",
    "    \"\"\"\n",
    "    L = len(parameters)\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads[\"dW\" + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#FF0000 size=4 face=\"黑体\">Adam</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize Adam\n",
    "def initialize_adam(parameters):\n",
    "    L = len(parameters)\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros(parameters[\"W\" + str(l + 1)].shape)\n",
    "        v[\"db\" + str(l + 1)] = np.zeros(parameters[\"b\" + str(l + 1)].shape)\n",
    "        s[\"dW\" + str(l + 1)] = np.zeros(parameters[\"W\" + str(l + 1)].shape)\n",
    "        s[\"db\" + str(l + 1)] = np.zeros(parameters[\"b\" + str(l + 1)].shape)\n",
    "        \n",
    "    return v, s\n",
    "\n",
    "# update parameters with Adam\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing updated parameters\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    \"\"\"\n",
    "    L = len(parameters)//2 #number of layers in the neural networks\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads[\"dW\" + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)]/(1 - np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)]/(1 - np.power(beta1, t))\n",
    "        \n",
    "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.multiply(grads[\"dW\" + str(l + 1)], grads[\"dW\" + str(l + 1)])\n",
    "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.multiply(grads[\"db\" + str(l + 1)], grads[\"db\" + str(l + 1)])\n",
    "        \n",
    "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)]/(1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)]/(1 - np.power(beta2, t))\n",
    "        \n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)]/(np.sqrt(s_corrected[\"dW\" + str(l + 1)]) + epsilon)\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)]/(np.sqrt(s_corrected[\"db\" + str(l + 1)]) + epsilon)\n",
    "        \n",
    "        return parameters, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#FF0000 size=4 face=\"黑体\">Model with different optimization algorithms</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "         beta1 = 0.9, beta2  = 0.999, epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
    "    \"\"\"\n",
    "    num_epochs -- number of epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing updated parameters\n",
    "    \"\"\"\n",
    "    L = len(layer_dims)\n",
    "    costs = []\n",
    "    t = 0\n",
    "    \n",
    "    parameters = Deep_Neural_Network.initialize_parameters(layer_dims)\n",
    "    \n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size)\n",
    "        (minibatch_X, minibatch_Y) = minibatch\n",
    "        \n",
    "        aL, caches = Deep_Neural_Network.L_model_forward(minibatch_X, parameters)\n",
    "        \n",
    "        cost = Deep_Neural_Network.compute_cost(aL, minibatch_Y)\n",
    "        \n",
    "        grads = Deep_Neural_Network.L_model_backward(aL, minibatch_Y, caches)\n",
    "        \n",
    "        if optimizer == \"gd\":\n",
    "            parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "        elif optimizer == \"momentum\":\n",
    "            parameters = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "        elif optimizer == \"adam\":\n",
    "            parameters = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\n",
    "            \n",
    "    if print_cost and i % 1000 == 0:\n",
    "        print(\"Cost after epoch %i: %f\" %(i, cost))\n",
    "    if print_cost and i % 100 == 0:\n",
    "        costs.append(cost)\n",
    "    \n",
    "    # plot\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
